{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/35156624/126909072-47c9be9e-549c-420f-ac4b-f9bbd2a4de22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to import the dataset for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw movie lines:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw movie lines:\")\n",
    "print()\n",
    "movie_lines[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw Conversations:\")\n",
    "print()\n",
    "conversations[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary to map movie line and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_movieline = {}\n",
    "for line in movie_lines:\n",
    "    _line = line.split(\" +++$+++ \")\n",
    "    if len(_line) == 5:\n",
    "        id_2_movieline[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie Lines of data set:\n",
      "\n",
      "{'L236278': 'They went crazy.', 'L328290': \"I'm not with the Agency, Mr. Garrison, and I assume if you've come this far, what I have to say interests you.  But I'm not going to name names, or tell you who or what I represent. Except to say - you're close, you're closer than you think...\", 'L278820': \"Oh, fine.  She's gone away for a little while and when she comes back I've sort of resolved to really tell her how much I care for her.\", 'L366322': \"I think it's fantastic.\", 'L506979': 'Can I tell you kids something?', 'L625440': \"The MCP is the most efficient way of handling what we do. I can't sit and worry about every little user request that --\", 'L395818': \"You're Mabel - her sister - aren't you?\", 'L236405': 'Did you bring that tape?', 'L633419': \"Didn't have a choice.  My car overheated up the road.\"}\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Movie Lines of data set:\")\n",
    "print()\n",
    "print(dict(list(id_2_movieline.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of all the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(\",\"))\n",
    "print()\n",
    "print(\"List of conversations:\")\n",
    "print()\n",
    "conversations_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the questions and answers\n",
      "\n",
      "Questions:\n",
      "\n",
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Why?', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', 'Gosh, if only we could find Kat a boyfriend...']\n",
      "\n",
      "Answers:\n",
      "\n",
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', \"That's a shame.\", 'Let me see what I can do.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Split the questions and answers\")\n",
    "print()\n",
    "questions = []\n",
    "answers = []\n",
    "for convs in conversations_ids:\n",
    "    for i in range(len(convs) - 1):\n",
    "        questions.append(id_2_movieline[convs[i]])\n",
    "        answers.append(id_2_movieline[convs[i + 1  ]])\n",
    "print(\"Questions:\")\n",
    "print()\n",
    "print(questions[:10])\n",
    "print()\n",
    "print(\"Answers:\")\n",
    "print()\n",
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    function: clean\n",
    "    params: String text\n",
    "    does: cleans the text removing stop words, punctuation, lower case.\n",
    "    returns: String clean text \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Questions:\n",
      "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again', 'well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', 'you are asking me out  that is so cute what is your name again', \"no no it's my fault  we didn't have a proper introduction \", 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'why', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'gosh if only we could find kat a boyfriend']\n",
      "\n",
      "Cleaned Answers:\n",
      "['well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', \"okay then how 'bout we try out some french cuisine  saturday  night\", 'forget it', 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'seems like she could get a date easy enough', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'that is a shame', 'let me see what i can do']\n"
     ]
    }
   ],
   "source": [
    "clean_ques = []\n",
    "clean_answ = []\n",
    "for question in questions:\n",
    "    clean_ques.append(clean(question))\n",
    "for answer in answers:\n",
    "    clean_answ.append(clean(answer))\n",
    "print()\n",
    "print(\"Cleaned Questions:\")\n",
    "print(clean_ques[:10])\n",
    "print()\n",
    "print(\"Cleaned Answers:\")\n",
    "print(clean_answ[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove less frequent words\n",
    "\n",
    "Find the number of occurunces of each word and remove the lowers 5%, this is to speed up the process of training the data in the neural network and to focus on the most impactful words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count hash table:\n",
      "\n",
      "{'marston': 1, 'usewe': 1, 'paw': 9, 'amscray': 2, 'started': 853, \"minutes'\": 3, 'hereor': 4, 'alleys': 11, 'motherfucker!!': 1}\n"
     ]
    }
   ],
   "source": [
    "count_words = {}\n",
    "for ques in clean_ques:\n",
    "    for word in ques.split():\n",
    "        if word in count_words:\n",
    "            count_words[word] += 1\n",
    "        else:\n",
    "            count_words[word] = 1\n",
    "\n",
    "for answ in clean_answ:\n",
    "    for word in answ.split():\n",
    "        if word in count_words:\n",
    "            count_words[word] += 1\n",
    "        else:\n",
    "            count_words[word] = 1\n",
    "print()\n",
    "print(\"Word count hash table:\")\n",
    "print()\n",
    "print(dict(list(count_words.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and create a threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize to get all words and filter out words that do not meet the threshold. The threshold is set at 20%, this hyperparamater can be attuned at different levels to improve the model. Map the words to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions Mapping:\n",
      "\n",
      "{'intellectual': 6723, \"man's\": 5513, 'insight': 1143, 'prize': 6489, 'started': 0, 'actions': 1144, 'sang': 4345, 'reconsider': 4344, 'studied': 7889}\n",
      "\n",
      "Answers Mapping\n",
      "\n",
      "{'intellectual': 6723, \"man's\": 5513, 'insight': 1143, 'prize': 6489, 'started': 0, 'actions': 1144, 'sang': 4345, 'reconsider': 4344, 'studied': 7889}\n"
     ]
    }
   ],
   "source": [
    "threshold = 20\n",
    "questions_mapping = {}\n",
    "w_count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count >= threshold:\n",
    "        questions_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "\n",
    "threshold_answ = 20\n",
    "answers_mapping = {}\n",
    "w_count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count >= threshold_answ:\n",
    "        answers_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "\n",
    "print()\n",
    "print(\"Questions Mapping:\")\n",
    "print()\n",
    "print(dict(list(questions_mapping.items())[1:10]))\n",
    "print()\n",
    "print(\"Answers Mapping\")\n",
    "print()\n",
    "print(dict(list(answers_mapping.items())[1:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: LEFT OFF HERE, WORKS ABOVE. \n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>','<SOS>']\n",
    "\n",
    "for token in tokens:\n",
    "    questions_mapping[token] = len(questions_mapping) + 1\n",
    "\n",
    "for token in tokens:\n",
    "    answers_mapping[token] = len(answers_mapping) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_answers = {w_i: w for w, w_i in answers_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add the EOS token to end of every answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_answ)):\n",
    "    clean_answ[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
       " 'not the hacking and gagging and spitting part  please <EOS>',\n",
       " \"okay then how 'bout we try out some french cuisine  saturday  night <EOS>\",\n",
       " 'forget it <EOS>',\n",
       " 'cameron <EOS>',\n",
       " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does <EOS>',\n",
       " 'seems like she could get a date easy enough <EOS>',\n",
       " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something <EOS>',\n",
       " 'that is a shame <EOS>',\n",
       " 'let me see what i can do <EOS>']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\")\n",
    "print()\n",
    "clean_answ[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the questions and answers for the RNN\n",
    "\n",
    "We need to map the questions and answers to integers in order to train the RNN. This is required as categorical data cannot be trained this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_mapping_ints = []\n",
    "\n",
    "for ques in clean_ques:\n",
    "    words_map = []\n",
    "    for word in ques.split():\n",
    "        if word not in questions_mapping:\n",
    "            words_map.append(questions_mapping['<OUT>'])\n",
    "        else:\n",
    "            words_map.append(questions_mapping[word])\n",
    "    quest_mapping_ints.append(words_map)\n",
    "    \n",
    "answ_mapping_ints = []\n",
    "\n",
    "for answ in clean_answ:\n",
    "    words_map = []\n",
    "    for word in answ.split():\n",
    "        if word not in answers_mapping:\n",
    "            words_map.append(answers_mapping['<OUT>'])\n",
    "        else:\n",
    "            words_map.append(answers_mapping[word])\n",
    "    answ_mapping_ints.append(words_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions Map to integers:\n",
      "\n",
      "[[7605, 3245, 6962, 984, 4142, 8824, 8824, 7071, 4137, 8824, 1948, 2507, 6739, 5454, 8824, 1882, 7415, 4290, 6632, 8247, 8824, 2082], [8556, 2487, 1274, 3245, 8429, 7266, 841, 8824, 8480, 5956, 5359, 3113, 841, 761], [4253, 8247, 8824, 7071, 8824, 7071, 8824, 8196, 8537], [761, 1948, 4787, 159, 5479, 5956, 5359, 5167, 1128, 615, 5359, 2647, 1081, 2082], [6895, 6895, 5244, 4015, 2517, 3245, 4184, 1737, 1828, 6338, 8824], [2774], [8247, 2796, 5359, 2774, 2487, 5435, 6903, 8247, 928, 2663, 1828, 6197, 8824, 5079, 2663, 7060, 4015, 8332, 2487, 5971, 697, 7668, 1838, 1693], [8345], [8824, 3775, 1838, 1142, 205, 6459, 1591, 702, 6423, 1838, 0, 5181, 959, 6932, 1044, 3398, 7794, 7792, 1838, 4745, 4871, 2663, 1044, 7991, 3623], [8021, 8480, 1954, 3245, 7778, 5670, 7767, 1828, 2296]]\n",
      "\n",
      "Answers Map to integers:\n",
      "\n",
      "[[8556, 2487, 1274, 3245, 8429, 7266, 841, 8824, 8480, 5956, 5359, 3113, 841, 761, 8823], [4253, 8247, 8824, 7071, 8824, 7071, 8824, 8196, 8537, 8823], [3113, 6932, 5977, 5255, 3245, 4028, 5479, 455, 6690, 8824, 3590, 4682, 8823], [7793, 1044, 8823], [2774, 8823], [8247, 2796, 5359, 2774, 2487, 5435, 6903, 8247, 928, 2663, 1828, 6197, 8824, 5079, 2663, 7060, 4015, 8332, 2487, 5971, 697, 7668, 1838, 1693, 8823], [716, 7792, 1838, 7778, 1942, 1828, 697, 3111, 2147, 8823], [8824, 3775, 1838, 1142, 205, 6459, 1591, 702, 6423, 1838, 0, 5181, 959, 6932, 1044, 3398, 7794, 7792, 1838, 4745, 4871, 2663, 1044, 7991, 3623, 8823], [5956, 5359, 1828, 594, 8823], [3866, 159, 3573, 615, 2487, 7605, 2, 8823]]\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Questions Map to integers:\")\n",
    "print()\n",
    "print(quest_mapping_ints[:10])\n",
    "print()\n",
    "print(\"Answers Map to integers:\")\n",
    "print()\n",
    "print(answ_mapping_ints[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to sort the questions and answers by the length of the questions. This will speed up the training in optimization stage. We can set the length of answer and question as 25, as a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted Questions: \n",
      "\n",
      "[[2774], [8345], [2521], [2450], [367], [6895], [3344], [6895], [1666], [3072]]\n",
      "\n",
      "Sorted Answers: \n",
      "\n",
      "[[8247, 2796, 5359, 2774, 2487, 5435, 6903, 8247, 928, 2663, 1828, 6197, 8824, 5079, 2663, 7060, 4015, 8332, 2487, 5971, 697, 7668, 1838, 1693, 8823], [8824, 3775, 1838, 1142, 205, 6459, 1591, 702, 6423, 1838, 0, 5181, 959, 6932, 1044, 3398, 7794, 7792, 1838, 4745, 4871, 2663, 1044, 7991, 3623, 8823], [8162, 8823], [6965, 7792, 612, 6537, 5479, 1184, 4605, 8823], [761, 6363, 7858, 984, 8019, 8823], [3113, 761, 1948, 4387, 1313, 205, 7255, 5977, 205, 3442, 8823], [191, 2441, 8823], [761, 5577, 2813, 7021, 1988, 1044, 8823], [5638, 8823], [8429, 761, 2352, 3691, 159, 1828, 76, 2774, 8823]]\n"
     ]
    }
   ],
   "source": [
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "for i in range(1, 26):\n",
    "    for indx, val in enumerate(quest_mapping_ints):\n",
    "        if len(val) == i:\n",
    "            sorted_questions.append(val)\n",
    "            sorted_answers.append(answ_mapping_ints[indx])\n",
    "\n",
    "print()\n",
    "print(\"Sorted Questions: \")\n",
    "print()\n",
    "print(sorted_questions[:10])\n",
    "print()\n",
    "print(\"Sorted Answers: \")\n",
    "print()\n",
    "print(sorted_answers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    function: model_inputs\n",
    "    params: none\n",
    "    returns: int inputs, int target, float learning rate, float drop_out\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    learning_rate = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    drop_out = tf.placeholder(tf.float32, name = 'drop_out')\n",
    "    return inputs, targets, learning_rate, drop_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_targets(targets, hash_words, batch_size):\n",
    "    \"\"\"\n",
    "    function: lstm_targets\n",
    "    params: targets tenor, hash_words hash table, batch_size int\n",
    "    returns: tensors targets\n",
    "    \"\"\"\n",
    "    left = tf.fill([batch_size, 1], hash_words['<SOS>'])\n",
    "    right = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    targets = tf.concat([left, right], 1)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(rnn_inputs, rnn_size, rnn_num_layers, drop_out, seq_len):\n",
    "    \"\"\"\n",
    "    function: encoder_layer\n",
    "    params: rnn_inputs, int size number input size, rnn_num_layers int, dropout rate int, seq_len\n",
    "    int length of list in batch\n",
    "    returns: encoder layer\n",
    "    \"\"\"\n",
    "    # make the lstm\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # apply dropout on lstm\n",
    "    dropout_lstm = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = drop_out)\n",
    "    # create the encoder cell\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([dropout_lstm] * rnn_num_layers)\n",
    "    # dynamic RNN\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, \n",
    "                                                       cell_bw = encoder_cell,\n",
    "                                                      sequence_length = seq_len,\n",
    "                                                      inputs = rnn_inputs,\n",
    "                                                      dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_training(encoder_state, decoder, decoder_input, seq_len, decode_scope, output_fun, drop_out, batch_size):\n",
    "    \"\"\"\n",
    "    function: decoder_training\n",
    "    params: encoder_state returned from encoder_layer, decoder cell in RNN, decoder_input embedding,\n",
    "    seq len int, decode scope, out output function, drop_out drop out rate, batch size int)\n",
    "    returns: decoder output with drop out\n",
    "    \"\"\"\n",
    "    states = tf.zeros([batch_size, 1, decoder.output_size])\n",
    "    keys, vals, scores, constructs = tf.contrib.seq2seq.prepare_attention(states, \n",
    "                                                                          attention_option = 'bahdanau',\n",
    "                                                                          num_units = decoder.output_size)\n",
    "\n",
    "    decoder_training = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], \n",
    "                                                                     keys, \n",
    "                                                                     vals, \n",
    "                                                                     scores, \n",
    "                                                                     constructs,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "\n",
    "    output, _final_state, _final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder,\n",
    "                                                                                        decoder_training,\n",
    "                                                                                        decoder_input, \n",
    "                                                                                        seq_len,\n",
    "                                                                                        scope = decode_scope)\n",
    "    decoder_drop_out = tf.nn.dropout(output, drop_out)\n",
    "    return  output_fun(decoder_drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_validation_set(encoder_state, decoder, decoder_matrix, sos_id, eos_id, max_len_batch, total_words_ans,\n",
    "                              decode_scope, output_fun, drop_out, batch_size):\n",
    "        \"\"\"\n",
    "        function: decode_validation_set\n",
    "        params: encoder_state returned from encoder_layer, decoder cell in RNN, decoder_input embedding,\n",
    "        seq len int, decode scope, out output function, drop_out drop out rate, batch size int\n",
    "        returns: test_predictions\n",
    "        \"\"\"\n",
    "        states = tf.zeros([batch_size, 1, decoder.output_size])\n",
    "        keys, vals, scores, constructs = tf.contrib.seq2seq.prepare_attention(states,\n",
    "                                                                              attention_option = 'bahdanau',\n",
    "                                                                              num_units = decoder.output_size)\n",
    "   \n",
    "        decoder_test = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fun,\n",
    "                                                                         encoder_state[0],\n",
    "                                                                         keys,\n",
    "                                                                         vals,\n",
    "                                                                         scores,\n",
    "                                                                         constructs,\n",
    "                                                                         decoder_matrix,\n",
    "                                                                         sos_id,\n",
    "                                                                         eos_id,\n",
    "                                                                         max_len_batch,\n",
    "                                                                         total_words_ans,\n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "   \n",
    "        test_predictions, _final_state, _final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder,\n",
    "                                                                                                      decoder_test, \n",
    "                                                                                                     scope = decode_scope)\n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smartbot_rnn(decoder_input, decoder_matrix, encoder_state, total_words, seq_len, rnn_size, num_layers_rnn,\n",
    "                    hash_words, drop_out, batch_size):\n",
    "        \"\"\"\n",
    "        function: smartbot_rnn\n",
    "        params: decoder_input, decoder_matrix, encoder_state, total_words_corpus int, seq_len int, rnn_size int,\n",
    "        num_layers_rnn int, hash_words hashtable, drop_out float, batch_size int\n",
    "        returns:\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "            lstm_layer_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = drop_out)\n",
    "            smartbot_decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_layer_dropout] * num_layers_rnn)\n",
    "            weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "            bias = tf.zeros_initializer()\n",
    "            output_fun = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                    total_words,\n",
    "                                                                    None,\n",
    "                                                                    scope = decoding_scope,\n",
    "                                                                    weights_initializer = weights,\n",
    "                                                                    biases_initializer = bias)\n",
    "            train_preds = decoder_training(encoder_state,\n",
    "                                         smartbot_decoder_cell,\n",
    "                                         decoder_input,\n",
    "                                         seq_len,\n",
    "                                         decoding_scope,\n",
    "                                         output_fun,\n",
    "                                         drop_out,\n",
    "                                         batch_size)\n",
    "   \n",
    "            decoding_scope.reuse_variables()\n",
    "            test_preds = decode_validation_set(encoder_state,\n",
    "                                                    smartbot_decoder_cell,\n",
    "                                                    decoder_matrix,\n",
    "                                                    hash_words['<SOS>'],\n",
    "                                                    hash_words['<EOS>'],\n",
    "                                                    seq_len - 1,\n",
    "                                                    total_words,\n",
    "                                                    decoding_scope,\n",
    "                                                    output_fun,\n",
    "                                                    drop_out,\n",
    "                                                    batch_size)\n",
    "   \n",
    "            return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smartbot_model(inputs, targets, dropout, batch_size, seq_len, num_words_answers, num_words_questions, encoder_size,\n",
    "                      decoder_size, rnn_size, rnn_num_layers, questions_hash):\n",
    "        \"\"\"\n",
    "        function: smartbot_model\n",
    "        params: inputs questions vector, targets answers vector, dropout rate float, batch_size int, seq_len int,\n",
    "                num_words_answers int, num_words_questions int, encoder_size int, decoder_size int, rnn_size int,\n",
    "                rnn_num_layers int, questions_hash hashtable\n",
    "        returns: seq2seq RNN model\n",
    "        \"\"\"\n",
    "        encoder_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                        num_words_answers + 1,\n",
    "                                                        encoder_size,\n",
    "                                                        initializer = tf.random_uniform_initializer(0, 1))\n",
    "   \n",
    "        encoder_state = encoder_layer(encoder_input,\n",
    "                                     rnn_size,\n",
    "                                     rnn_num_layers,\n",
    "                                     dropout,\n",
    "                                     seq_len)\n",
    "   \n",
    "        targets = lstm_targets(targets, questions_mapping, batch_size)\n",
    "        decoder_matrix = tf.Variable(tf.random_uniform([num_words_questions + 1, decoder_size], 0, 1))\n",
    "        decoder_input = tf.nn.embedding_lookup(decoder_matrix, targets)\n",
    "        \n",
    "        train_preds, test_preds = smartbot_rnn(decoder_input,\n",
    "                                               decoder_matrix,\n",
    "                                               encoder_state,\n",
    "                                               num_words_questions, \n",
    "                                               seq_len,\n",
    "                                               rnn_size, \n",
    "                                               rnn_num_layers,\n",
    "                                               questions_mapping,\n",
    "                                               dropout,\n",
    "                                               batch_size)\n",
    "        return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model - Set up the hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "number_layers = 3\n",
    "encoding_embed_size = 512\n",
    "decoding_embed_size = 512\n",
    "learning_rate = 0.01\n",
    "# percent learning rate is reduced, learn in more depth as it progresses. Commmon value is 90%\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tensorflow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, learning, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 max length no more than 25 words in questions and answers, no tensor to deal with, name of seq length\n",
    "seq_len = tf.placeholder_with_default(25, None, name = \"sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start outputting the training and test predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, test_preds = smartbot_model(tf.reverse(inputs, [-1]), \n",
    "                                                    targets, \n",
    "                                                    keep_prob, \n",
    "                                                    batch_size, \n",
    "                                                    seq_len,\n",
    "                                                    len(answers_mapping),\n",
    "                                                    len(questions_mapping),\n",
    "                                                    encoding_embed_size,\n",
    "                                                    decoding_embed_size,\n",
    "                                                    rnn_size,\n",
    "                                                    number_layers,\n",
    "                                                    questions_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the loss Error and Optimizes. Apply gradient clipping to the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(train_preds, \n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], seq_len]))\n",
    "    optim = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optim.compute_gradients(loss_error)\n",
    "    clipped_grads = [(tf.clip_by_value(tensor, -.5, -.5), var) for tensor, var in gradients if tensor is not None]\n",
    "    optim_gradient_clip = optim.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(batch_seqs, hash_words_ints):\n",
    "    \"\"\"\n",
    "    function: padding\n",
    "    params: batch seqs, hash_words_ints hash table words to integers\n",
    "    returns: sequence with <PAD> token\n",
    "    does: Complete sentences with pad tokens, so all tokens have the same length\n",
    "    \"\"\"\n",
    "    max_seq_len = max([len(seqs) for seqs in batch_seqs])\n",
    "    return [seq + [hash_words_ints[\"<PAD>\"]] * (max_seq_len - len(seq)) for seq in batch_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data to batches of questions & answers\n",
    "def split_into_batches(ques, ans, batch_size):\n",
    "    \"\"\"\n",
    "    function: split_into_batches\n",
    "    params: list ques, list ans, int batch size\n",
    "    does: splits data into batches\n",
    "    returns: batches of data\n",
    "    \"\"\"\n",
    "    for batch_index in range(0, len(ques) // batch_size):\n",
    "        begin_indx = batch_index * batch_size\n",
    "        ques_batch = ques[begin_indx : begin_indx + batch_size]\n",
    "        answ_batch = answ[begin_indx : begin_indx + batch_size]\n",
    "        padded_ques = np.array(padding(ques, questions_mapping))\n",
    "        padded_answ = np.array(padding(answ, answers_mapping))\n",
    "        yield padded_ques, padded_answ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation, for training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = int(len(sorted_questions) * 0.15)\n",
    "training_ques = sorted_questions[training_split:]\n",
    "training_answ = sorted_answers[training_split:]\n",
    "validation_ques = sorted_questions[:training_split]\n",
    "validation_answ = sorted_answers[:training_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## left off here \n",
    "training_loss_indx = 100\n",
    "validation_loss_indx = ((len(training_ques)) // batch_size // 2) - 1\n",
    "loss_error = 0\n",
    "loss_error_list = []\n",
    "stop_check = 0\n",
    "stop = 1000\n",
    "checkpoint = \"Chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_indx, (padded_quest, padded_answ) in enumerate(split_into_batches(training_ques, \n",
    "                                                                                training_answ,\n",
    "                                                                                batch_size)):\n",
    "        start_time = time.time()\n",
    "        _, error = session_run([optim_gradient_clip, loss_error], \n",
    "                              {inputs: padded_quest, \n",
    "                              learning_rate: learning_rate,\n",
    "                              sequence_length: padded_answ[1]}, \n",
    "                              keep_prob: keep_probability)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
