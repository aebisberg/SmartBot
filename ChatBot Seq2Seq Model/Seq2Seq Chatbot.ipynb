{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/35156624/126909072-47c9be9e-549c-420f-ac4b-f9bbd2a4de22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to import the dataset for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw movie lines:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw movie lines:\")\n",
    "print()\n",
    "movie_lines[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw Conversations:\")\n",
    "print()\n",
    "conversations[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary to map movie line and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_movieline = {}\n",
    "for line in movie_lines:\n",
    "    _line = line.split(\" +++$+++ \")\n",
    "    if len(_line) == 5:\n",
    "        id_2_movieline[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie Lines of data set:\n",
      "\n",
      "{'L454353': 'Mary had been there, one night, and had left.', 'L334710': \"It's Jennie. Just tell me if Telly is there.\", 'L86783': 'I just know how you get. Good to know, them butterflies still in ya gut.', 'L103759': 'Was Future Man adopted?', 'L633072': \"You're not talking sense.\", 'L642272': 'She looks like a sick marrow!', 'L422114': \"How's that, Sheriff?\", 'L144387': \"Daryll Lee Cullum?  I don't think so.  If he's escaped we'd have the National Guard, cops'd be crawling through sewers.  You'd have a guard on your front door.\", 'L413490': 'John: take that look offen your face and act nice.'}\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Movie Lines of data set:\")\n",
    "print()\n",
    "print(dict(list(id_2_movieline.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of all the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(\",\"))\n",
    "print()\n",
    "print(\"List of conversations:\")\n",
    "print()\n",
    "conversations_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the questions and answers\n",
      "\n",
      "Questions:\n",
      "\n",
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Why?', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', 'Gosh, if only we could find Kat a boyfriend...']\n",
      "\n",
      "Answers:\n",
      "\n",
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', \"That's a shame.\", 'Let me see what I can do.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Split the questions and answers\")\n",
    "print()\n",
    "questions = []\n",
    "answers = []\n",
    "for convs in conversations_ids:\n",
    "    for i in range(len(convs) - 1):\n",
    "        questions.append(id_2_movieline[convs[i]])\n",
    "        answers.append(id_2_movieline[convs[i + 1  ]])\n",
    "print(\"Questions:\")\n",
    "print()\n",
    "print(questions[:10])\n",
    "print()\n",
    "print(\"Answers:\")\n",
    "print()\n",
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    function: clean\n",
    "    params: String text\n",
    "    does: cleans the text removing stop words, punctuation, lower case.\n",
    "    returns: String clean text \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=-[.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Questions:\n",
      "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again', 'well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', 'you are asking me out  that is so cute what is your name again', \"no no it's my fault  we didn't have a proper introduction \", 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'why', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'gosh if only we could find kat a boyfriend']\n",
      "\n",
      "Cleaned Answers:\n",
      "['well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', \"okay then how 'bout we try out some french cuisine  saturday  night\", 'forget it', 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'seems like she could get a date easy enough', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'that is a shame', 'let me see what i can do']\n"
     ]
    }
   ],
   "source": [
    "clean_ques = []\n",
    "clean_answ = []\n",
    "for question in questions:\n",
    "    clean_ques.append(clean(question))\n",
    "for answer in answers:\n",
    "    clean_answ.append(clean(answer))\n",
    "print()\n",
    "print(\"Cleaned Questions:\")\n",
    "print(clean_ques[:10])\n",
    "print()\n",
    "print(\"Cleaned Answers:\")\n",
    "print(clean_answ[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove less frequent words\n",
    "\n",
    "Find the number of occurunces of each word and remove the lowers 5%, this is to speed up the process of training the data in the neural network and to focus on the most impactful words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count hash table:\n",
      "\n",
      "{'scumbag': 38, 'duty': 213, 'foundation!': 2, 'heaving': 4, 'doggonedest': 1, 'areenchanting': 1, 'onsite': 4, 'hotspur': 1, 'successteamwork': 1}\n"
     ]
    }
   ],
   "source": [
    "count_words = {}\n",
    "for ques in clean_ques:\n",
    "    for word in ques.split():\n",
    "        if word in count_words:\n",
    "            count_words[word] += 1\n",
    "        else:\n",
    "            count_words[word] = 1\n",
    "\n",
    "for answ in clean_answ:\n",
    "    for word in answ.split():\n",
    "        if word in count_words:\n",
    "            count_words[word] += 1\n",
    "        else:\n",
    "            count_words[word] = 1\n",
    "print()\n",
    "print(\"Word count hash table:\")\n",
    "print()\n",
    "print(dict(list(count_words.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and create a threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize to get all words and filter out words that do not meet the threshold. The threshold is set at 20%, this hyperparamater can be attuned at different levels to improve the model. Map the words to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions Mapping:\n",
      "\n",
      "{'publish': 4295, 'parasites': 2220, 'scumbag': 0, 'duty': 1, 'mallory': 6852, 'federation': 5436, 'moraes': 6508, 'kinky': 8466, 'brady': 6504}\n",
      "\n",
      "Answers Mapping\n",
      "\n",
      "{'publish': 12831, 'parasites': 10756, 'scumbag': 8536, 'duty': 8537, 'mallory': 15388, 'federation': 13972, 'moraes': 15044, 'kinky': 17002, 'brady': 15040}\n"
     ]
    }
   ],
   "source": [
    "threshold = 20\n",
    "questions_mapping = {}\n",
    "w_count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count > threshold:\n",
    "        questions_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "        \n",
    "answers_mapping = {}\n",
    "count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count > threshold:\n",
    "        answers_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "\n",
    "print()\n",
    "print(\"Questions Mapping:\")\n",
    "print()\n",
    "print(dict(list(questions_mapping.items())[1:10]))\n",
    "print()\n",
    "print(\"Answers Mapping\")\n",
    "print()\n",
    "print(dict(list(answers_mapping.items())[1:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>','<SOS>']\n",
    "\n",
    "for token in tokens:\n",
    "    questions_mapping[token] = len(questions_mapping) + 1\n",
    "\n",
    "for token in tokens:\n",
    "    answers_mapping[token] = len(answers_mapping) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_answers = {w_i: w for w, w_i in answers_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add the EOS token to end of every answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_answ)):\n",
    "    clean_answ[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
       " 'not the hacking and gagging and spitting part  please <EOS>',\n",
       " \"okay then how 'bout we try out some french cuisine  saturday  night <EOS>\",\n",
       " 'forget it <EOS>',\n",
       " 'cameron <EOS>',\n",
       " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does <EOS>',\n",
       " 'seems like she could get a date easy enough <EOS>',\n",
       " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something <EOS>',\n",
       " 'that is a shame <EOS>',\n",
       " 'let me see what i can do <EOS>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\")\n",
    "print()\n",
    "clean_answ[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
